{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"UNSLOTH_DISABLE_FAST_GENERATION\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from unsloth import FastModel\n",
    "from borealis.modeling import BorealisForConditionalGeneration\n",
    "from borealis.dataset import BorealisBaseDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperFeatureExtractor, Qwen2ForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, Audio\n",
    "from jiwer import wer, cer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff55c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    dtype=None,\n",
    "    auto_model=Qwen2ForCausalLM,\n",
    "    full_finetuning=True,\n",
    ")\n",
    "\n",
    "start_audio_token = \"<|start_of_audio|>\"\n",
    "end_audio_token = \"<|end_of_audio|>\"\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [start_audio_token, end_audio_token]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Vikhrmodels/ToneSpeak\")\n",
    "ds = ds.cast_column(\"audio\", Audio(decode=True, sampling_rate=16_000))\n",
    "\n",
    "whisper_encoder = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BorealisForConditionalGeneration(\n",
    "    language_model=language_model, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\"/workspace/Borealis/asr_qwen_ckpts/checkpoint-163525/pytorch_model.bin\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = BorealisBaseDataset(\n",
    "    audio_processor=whisper_encoder,\n",
    "    text_tokenizer=tokenizer,\n",
    "    hf_ds=ds[\"validation\"].select(range(79)),\n",
    "    max_text_len=320,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474874b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_transcripts = []\n",
    "ground_truth_texts = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(eval_dataset, desc=\"Inference on eval set\"):\n",
    "        mel = batch[\"mel\"].to(model.encoder.device)  # (B, 128, 3000)\n",
    "        att_mask = batch[\"audio_att_mask\"].to(model.encoder.device)  # (B, 3000)\n",
    "\n",
    "        transcripts = model.generate(\n",
    "            mel=mel,\n",
    "            att_mask=att_mask,\n",
    "            max_new_tokens=320,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "        )\n",
    "        print(transcripts)\n",
    "\n",
    "        gt_texts = tokenizer.decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        print(gt_texts)\n",
    "\n",
    "        generated_transcripts.extend(transcripts)\n",
    "        ground_truth_texts.extend(gt_texts)\n",
    "\n",
    "\n",
    "for i in range(min(5, len(generated_transcripts))):\n",
    "    print(f\"Сгенерировано: {generated_transcripts[i]}\")\n",
    "    print(f\"Правильный текст: {ground_truth_texts[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_single_wav(\n",
    "    wav_path: str,\n",
    "    model: BorealisForConditionalGeneration,\n",
    "    audio_processor: WhisperFeatureExtractor,\n",
    "    max_seconds_len: float = 30.0,\n",
    "    sampling_rate: int = 16000,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    generation_params: dict = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Загружает один WAV файл, обрабатывает его и генерирует транскрипт с помощью модели.\n",
    "\n",
    "    :param wav_path: Путь к WAV файлу.\n",
    "    :param model: Инстанс WhisperQWenASRModel.\n",
    "    :param audio_processor: WhisperFeatureExtractor для обработки аудио.\n",
    "    :param max_seconds_len: Максимальная длина аудио в секундах (для паддинга).\n",
    "    :param sampling_rate: Частота дискретизации (по умолчанию 16000).\n",
    "    :param device: Устройство ('cuda' или 'cpu').\n",
    "    :param generation_params: Словарь с параметрами для model.generate (например, {'temperature': 0.0, 'max_new_tokens': 320}).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    waveform, sr = librosa.load(wav_path, sr=None)  # sr=None to load native sample rate\n",
    "    if sr != sampling_rate:\n",
    "        # Resample to the desired sampling rate\n",
    "        waveform = librosa.resample(waveform, orig_sr=sr, target_sr=sampling_rate)\n",
    "\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = np.mean(waveform, axis=0)\n",
    "\n",
    "    proc = audio_processor(\n",
    "        waveform,\n",
    "        sampling_rate=sampling_rate,\n",
    "        padding=\"max_length\",\n",
    "        max_length=int(max_seconds_len * sampling_rate),\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    mel = proc.input_features.squeeze(0).to(device)  # (80, 3000)\n",
    "    att_mask = proc.attention_mask.squeeze(0).to(device)  # (3000)\n",
    "\n",
    "    if generation_params is None:\n",
    "        generation_params = {\n",
    "            \"max_new_tokens\": 320,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        transcript = model.generate(mel=mel, att_mask=att_mask, **generation_params)\n",
    "\n",
    "    print(f\"Generated transcript for {wav_path}:\")\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_wav(\n",
    "    wav_path=\"/workspace/res.wav\",\n",
    "    model=model,\n",
    "    audio_processor=whisper_encoder,\n",
    "    generation_params={\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 320,\n",
    "        \"temperature\": 0.95,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41607894",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = load_dataset(\"Vikhrmodels/ToneRuLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5997be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c217947",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = BorealisBaseDataset(\n",
    "    audio_processor=whisper_encoder,\n",
    "    text_tokenizer=tokenizer,\n",
    "    hf_ds=test_ds[\"validation\"].select(range(79)),\n",
    "    max_text_len=320,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_transcripts = []\n",
    "ground_truth_texts = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(eval_dataset, desc=\"Inference on eval set\"):\n",
    "        mel = batch[\"mel\"].to(model.encoder.device)  # (B, 128, 3000)\n",
    "        att_mask = batch[\"audio_att_mask\"].to(model.encoder.device)  # (B, 3000)\n",
    "\n",
    "        transcripts = model.generate(\n",
    "            mel=mel,\n",
    "            att_mask=att_mask,\n",
    "            max_new_tokens=320,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "        )\n",
    "\n",
    "        gt_texts = tokenizer.decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        generated_transcripts.append(transcripts)\n",
    "        ground_truth_texts.append(gt_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_content(text: str) -> str:\n",
    "    if \"assistant\\n\" in text:\n",
    "        return text.split(\"assistant\\n\")[-1].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941bfa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text_list(text_list):\n",
    "    # Создаем множество символов пунктуации\n",
    "    punct = set(string.punctuation)\n",
    "\n",
    "    # Обрабатываем каждый текст в списке\n",
    "    cleaned_list = [\n",
    "        \"\".join(char for char in text.lower() if char not in punct)\n",
    "        for text in text_list\n",
    "    ]\n",
    "\n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_texts = [extract_assistant_content(text) for text in ground_truth_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_score = wer(\n",
    "    clean_text_list(ground_truth_texts), clean_text_list(generated_transcripts)\n",
    ")\n",
    "cer_score = cer(\n",
    "    clean_text_list(ground_truth_texts), clean_text_list(generated_transcripts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64633c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4624861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f033153d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d881fd0bf8e447a4a8b192887dd4afb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bench_set = load_dataset(\"Vikhrmodels/RuASRBenchmark\", num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6ea36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    Russian_LibriSpeech: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 1352\n",
       "    })\n",
       "    Common_Voice_Corpus_22.0: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 10244\n",
       "    })\n",
       "    Tone_Webinars: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 21587\n",
       "    })\n",
       "    Tone_Books: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 4930\n",
       "    })\n",
       "    Tone_Speak: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "    Sova_RuDevices: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 5799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_list(text_list):\n",
    "    punct = set(string.punctuation)\n",
    "\n",
    "    cleaned_list = [\n",
    "        \"\".join(char for char in text.lower() if char not in punct)\n",
    "        for text in text_list\n",
    "    ]\n",
    "\n",
    "    return cleaned_list\n",
    "\n",
    "def extract_assistant_content(text: str) -> str:\n",
    "    if \"assistant\\n\" in text:\n",
    "        return text.split(\"assistant\\n\")[-1].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Настройка DataLoader для батчей\n",
    "batch_size = 64  # Укажите размер батча\n",
    "for split in bench_set:\n",
    "    print(f\"Подсчёт метрик на сплите {split}\")\n",
    "\n",
    "    # Создаем DataLoader с батчами\n",
    "    test_loader = DataLoader(\n",
    "        BorealisBaseDataset(\n",
    "            audio_processor=whisper_encoder,\n",
    "            text_tokenizer=tokenizer,\n",
    "            hf_ds=bench_set[split],\n",
    "            max_text_len=320,\n",
    "        ),\n",
    "        batch_size=batch_size,  # Устанавливаем размер батча\n",
    "        shuffle=False,  # Не перемешиваем данные для теста\n",
    "        num_workers=16,  # Параллельная загрузка данных (настройте по вашему оборудованию)\n",
    "        pin_memory=True,  # Ускоряет передачу данных на GPU\n",
    "    )\n",
    "\n",
    "    ground_truth_texts = []\n",
    "    generated_transcripts = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_loader, desc=f\"Processing split {split}\"):\n",
    "            # Перемещаем батч на устройство модели\n",
    "            mel = batch[\"mel\"].to(model.encoder.device)  # (B, 128, 3000)\n",
    "            att_mask = batch[\"audio_att_mask\"].to(model.encoder.device)  # (B, 3000)\n",
    "\n",
    "            # Генерация транскриптов для всего батча\n",
    "            transcripts = model.generate(\n",
    "                mel=mel,\n",
    "                att_mask=att_mask,\n",
    "                max_new_tokens=320,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "            )\n",
    "\n",
    "            # Декодируем ground truth тексты для всего батча\n",
    "            gt_texts = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "            # Добавляем тексты в списки\n",
    "            ground_truth_texts.extend(\n",
    "                [extract_assistant_content(text) for text in gt_texts]\n",
    "            )\n",
    "            generated_transcripts.extend(transcripts)\n",
    "\n",
    "    # Вычисляем метрики\n",
    "    wer_score = wer(\n",
    "        clean_text_list(ground_truth_texts), clean_text_list(generated_transcripts)\n",
    "    )\n",
    "    cer_score = cer(\n",
    "        clean_text_list(ground_truth_texts), clean_text_list(generated_transcripts)\n",
    "    )\n",
    "\n",
    "    print(f\"WER score for split {split}: {wer_score:.4f}\")\n",
    "    print(f\"CER score for split {split}: {cer_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43adb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdeebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f1182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def test_random_sample(dataset):\n",
    "    if len(dataset) == 0:\n",
    "        print(\"Датасет пустой!\")\n",
    "        return\n",
    "\n",
    "    index = random.randint(0, len(dataset) - 1)\n",
    "    print(f\"Тестируем сэмпл с индексом: {index}\")\n",
    "\n",
    "    try:\n",
    "        sample = dataset[index]\n",
    "        print(\"Сэмпл успешно загружен!\")\n",
    "        print(\"Ключи в сэмпле:\", list(sample.keys()))\n",
    "\n",
    "        # Печать форм (для отладки)\n",
    "        print(\"Форма mel:\", sample[\"mel\"].shape)\n",
    "        print(\"Форма audio_att_mask:\", sample[\"audio_att_mask\"].shape)\n",
    "        print(\"Форма labels:\", sample[\"labels\"].shape)\n",
    "        print(\"Форма text_att_mask:\", sample[\"text_att_mask\"].shape)\n",
    "\n",
    "        # Опционально: декодировать текст для проверки\n",
    "        decoded_text = dataset.tokenizer.decode(\n",
    "            sample[\"labels\"], skip_special_tokens=True\n",
    "        )\n",
    "        print(\"Декодированный текст:\", decoded_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке сэмпла {index}: {e}\")\n",
    "        # Если ошибка в конкретном файле, можно добавить больше отладки\n",
    "        if \"path\" in dataset.audios[index]:\n",
    "            print(\"Путь к аудио:\", dataset.audios[index][\"path\"])\n",
    "\n",
    "\n",
    "# Запуск теста\n",
    "test_random_sample(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca11fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
